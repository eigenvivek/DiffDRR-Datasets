{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepfluoro\n",
    "\n",
    "> `DiffDRR` wrapper for the `DeepFluoro` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp deepfluoro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffdrr.data import read\n",
    "from diffdrr.pose import RigidTransform\n",
    "from diffdrr.utils import parse_intrinsic_matrix\n",
    "from torchio import LabelMap, ScalarImage, Subject\n",
    "from torchio.transforms.preprocessing import ToCanonical\n",
    "from torchvision.transforms.functional import center_crop, gaussian_blur\n",
    "\n",
    "from diffdrrdata.utils import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`DeepFluoro`](https://github.com/rg2/DeepFluoroLabeling-IPCAI2020) dataset is a collection of pelvic CT and X-ray images from 6 cadaveric subjects. \n",
    "For each subject, there is\n",
    "\n",
    "- One CT volume\n",
    "- 24-111 X-ray fluoroscopy images\n",
    "\n",
    "In total, the dataset comprises six CT volumes and 366 X-ray images (with ground truth camera poses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeepFluoroDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A `torch.utils.data.Dataset` that stores the imaging data for subjects\n",
    "    in the `DeepFluoro` dataset and provides an iterator over the X-ray\n",
    "    fluoroscopy images and associated poses for each subject. Imaging data\n",
    "    can be passed to a `diffdrr.drr.DRR` to renderer DRRs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        id_number: int,  # Subject ID in {1, ..., 6}\n",
    "        preprocess: bool = True,  # Preprocess raw X-rays\n",
    "        bone_attenuation_multiplier: float = 1.0,  # Scalar multiplier on density of high attenuation voxels\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the subject\n",
    "        (\n",
    "            self.subject,\n",
    "            self.projections,\n",
    "            self.anatomical2world,\n",
    "            self.world2camera,\n",
    "            self.focal_len,\n",
    "            self.height,\n",
    "            self.width,\n",
    "            self.delx,\n",
    "            self.dely,\n",
    "            self.x0,\n",
    "            self.y0,\n",
    "        ) = load(id_number, bone_attenuation_multiplier)\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        if self.preprocess:\n",
    "            self.height -= 100\n",
    "            self.width -= 100\n",
    "\n",
    "        # Miscellaneous transformation matrices for wrangling SE(3) poses\n",
    "        self.flip_z = RigidTransform(\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    [0, -1, 0, 0],\n",
    "                    [1, 0, 0, 0],\n",
    "                    [0, 0, -1, 0],\n",
    "                    [0, 0, 0, 1],\n",
    "                ]\n",
    "            ).to(torch.float32)\n",
    "        )\n",
    "        self.rot_180 = RigidTransform(\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    [-1, 0, 0, 0],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 1, 0],\n",
    "                    [0, 0, 0, 1],\n",
    "                ]\n",
    "            ).to(torch.float32)\n",
    "        )\n",
    "        self.reorient = RigidTransform(self.subject.reorient)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.projections)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self[idx] for idx in range(len(self)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.from_numpy(self.projections[f\"{idx:03d}/image/pixels\"][:])\n",
    "        pose = self.projections[f\"{idx:03d}/gt-poses/cam-to-pelvis-vol\"][:]\n",
    "        pose = RigidTransform(torch.from_numpy(pose))\n",
    "        pose = (\n",
    "            self.flip_z.compose(self.world2camera.inverse())\n",
    "            .compose(pose)\n",
    "            .compose(self.anatomical2world)\n",
    "        )\n",
    "        if self.rot_180_for_up(idx):\n",
    "            img = torch.rot90(img, k=2)\n",
    "            pose = self.rot_180.compose(pose)\n",
    "        pose = self.reorient.inverse().compose(pose)\n",
    "        img = img.unsqueeze(0).unsqueeze(0)\n",
    "        if self.preprocess:\n",
    "            img = preprocess(img)\n",
    "        return img, pose\n",
    "\n",
    "    def rot_180_for_up(self, idx):\n",
    "        return self.projections[f\"{idx:03d}/rot-180-for-up\"][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def parse_volume(subject, bone_attenuation_multiplier):\n",
    "    # Get all parts of the volume\n",
    "    volume = subject[\"vol/pixels\"][:]\n",
    "    volume = np.swapaxes(volume, 0, 2).copy()\n",
    "    volume = torch.from_numpy(volume).unsqueeze(0)\n",
    "\n",
    "    mask = subject[\"vol-seg/image/pixels\"][:]\n",
    "    mask = np.swapaxes(mask, 0, 2).copy()\n",
    "    mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "    affine = np.eye(4)\n",
    "    affine[:3, :3] = subject[\"vol/dir-mat\"][:]\n",
    "    affine[:3, 3:] = subject[\"vol/origin\"][:]\n",
    "    affine = torch.from_numpy(affine).to(torch.float32)\n",
    "\n",
    "    defns = subject[\"vol-seg/labels-def\"]\n",
    "    defns = {idx: defns[f\"{idx}\"][()].decode() for idx in range(1, len(defns) + 1)}\n",
    "\n",
    "    fiducials = torch.stack(\n",
    "        [\n",
    "            torch.from_numpy(subject[f\"vol-landmarks/{key}\"][()])\n",
    "            for key in subject[\"vol-landmarks\"].keys()\n",
    "        ]\n",
    "    ).permute(2, 0, 1)\n",
    "\n",
    "    volume = ScalarImage(tensor=volume, affine=affine)\n",
    "    labelmap = LabelMap(tensor=mask, affine=affine)\n",
    "\n",
    "    # Package the subject\n",
    "    subject = read(\n",
    "        volume=volume,\n",
    "        labelmap=labelmap,\n",
    "        bone_attenuation_multiplier=bone_attenuation_multiplier,\n",
    "        label_def=defns,\n",
    "        fiducials=fiducials,\n",
    "    )\n",
    "\n",
    "    # Move the fiducials's isocenter to the origin in world coordinates\n",
    "    isocenter = volume.get_center()\n",
    "    anatomical2world = RigidTransform(\n",
    "        torch.tensor(\n",
    "            [\n",
    "                [1.0, 0.0, 0.0, -isocenter[0]],\n",
    "                [0.0, 1.0, 0.0, -isocenter[1]],\n",
    "                [0.0, 0.0, 1.0, -isocenter[2]],\n",
    "                [0.0, 0.0, 0.0, 1.0],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return subject, anatomical2world\n",
    "\n",
    "\n",
    "def parse_proj_params(f):\n",
    "    proj_params = f[\"proj-params\"]\n",
    "    extrinsic = torch.from_numpy(proj_params[\"extrinsic\"][:])\n",
    "    world2camera = RigidTransform(extrinsic)\n",
    "    intrinsic = torch.from_numpy(proj_params[\"intrinsic\"][:])\n",
    "    num_cols = proj_params[\"num-cols\"][()]\n",
    "    num_rows = proj_params[\"num-rows\"][()]\n",
    "    proj_col_spacing = float(proj_params[\"pixel-col-spacing\"][()])\n",
    "    proj_row_spacing = float(proj_params[\"pixel-row-spacing\"][()])\n",
    "    return (\n",
    "        intrinsic,\n",
    "        world2camera,\n",
    "        num_cols,\n",
    "        num_rows,\n",
    "        proj_col_spacing,\n",
    "        proj_row_spacing,\n",
    "    )\n",
    "\n",
    "\n",
    "def load(id_number, bone_attenuation_multiplier):\n",
    "    f = load_file(\"ipcai_2020_full_res_data.h5\")\n",
    "\n",
    "    # Load dataset parameters\n",
    "    (\n",
    "        intrinsic,\n",
    "        world2camera,\n",
    "        num_cols,\n",
    "        num_rows,\n",
    "        proj_col_spacing,\n",
    "        proj_row_spacing,\n",
    "    ) = parse_proj_params(f)\n",
    "\n",
    "    focal_len, x0, y0 = parse_intrinsic_matrix(\n",
    "        intrinsic,\n",
    "        num_rows,\n",
    "        num_cols,\n",
    "        proj_row_spacing,\n",
    "        proj_col_spacing,\n",
    "    )\n",
    "\n",
    "    # Load subject data\n",
    "    assert id_number in {1, 2, 3, 4, 5, 6}\n",
    "    subject_id = [\n",
    "        \"17-1882\",\n",
    "        \"18-1109\",\n",
    "        \"18-0725\",\n",
    "        \"18-2799\",\n",
    "        \"18-2800\",\n",
    "        \"17-1905\",\n",
    "    ][id_number - 1]\n",
    "    subject = f[subject_id]\n",
    "    projections = subject[\"projections\"]\n",
    "    subject, anatomical2world = parse_volume(subject, bone_attenuation_multiplier)\n",
    "\n",
    "    return (\n",
    "        subject,\n",
    "        projections,\n",
    "        anatomical2world,\n",
    "        world2camera,\n",
    "        focal_len,\n",
    "        int(num_rows),\n",
    "        int(num_cols),\n",
    "        proj_row_spacing,\n",
    "        proj_col_spacing,\n",
    "        x0,\n",
    "        y0,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(img, size=None, initial_energy=torch.tensor(65487.0)):\n",
    "    \"\"\"\n",
    "    Recover the line integral: $L[i,j] = \\log I_0 - \\log I_f[i,j]$\n",
    "\n",
    "    (1) Remove edge due to collimator\n",
    "    (2) Smooth the image to make less noisy\n",
    "    (3) Subtract the log initial energy for each ray\n",
    "    (4) Recover the line integral image\n",
    "    (5) Rescale image to [0, 1]\n",
    "    \"\"\"\n",
    "    img = center_crop(img, (1436, 1436))\n",
    "    img = gaussian_blur(img, (5, 5), sigma=1.0)\n",
    "    img = initial_energy.log() - img.log()\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functionalities\n",
    "\n",
    "Below, we show\n",
    "\n",
    "- How to load the `DeepFluoro` dataset\n",
    "- How to create a `diffdrr.drr.DRR` object from the `DeepFluoro` dataset\n",
    "- A visualization of all the camera poses for all 6 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista\n",
    "from diffdrr.drr import DRR\n",
    "from diffdrr.visualization import _make_camera_frustum_mesh, labelmap_to_mesh\n",
    "from IPython.display import IFrame\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Labeled Surface Extraction: 100%|█████████████████████████████████████████████████████████████[00:00<00:00]\n",
      "Smoothing Mesh using Taubin Smoothing: 100%|█████████████████████████████████████████████████████████████[00:00<00:00]\n",
      "Cleaning: 100%|██████████████████████████████████████████████████████████████████████████████████████████[00:00<00:00]\n",
      "Subject 1: 100%|████████████████████████████████████████████████████████████████████| 111/111 [00:10<00:00, 11.03it/s]\n",
      "Subject 2: 100%|████████████████████████████████████████████████████████████████████| 104/104 [00:08<00:00, 11.69it/s]\n",
      "Subject 3: 100%|██████████████████████████████████████████████████████████████████████| 24/24 [00:02<00:00, 11.26it/s]\n",
      "Subject 4: 100%|██████████████████████████████████████████████████████████████████████| 48/48 [00:03<00:00, 12.38it/s]\n",
      "Subject 5: 100%|██████████████████████████████████████████████████████████████████████| 55/55 [00:04<00:00, 11.73it/s]\n",
      "Subject 6: 100%|██████████████████████████████████████████████████████████████████████| 24/24 [00:02<00:00, 11.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"749\"\n",
       "            height=\"500\"\n",
       "            src=\"renders/deepfluoro.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| eval: false\n",
    "plotter = pyvista.Plotter()\n",
    "\n",
    "\n",
    "colors = [\"#66c2a5\", \"#fc8d62\", \"#8da0cb\", \"#e78ac3\", \"#a6d854\", \"#ffd92f\"]\n",
    "for idx in range(1, 7):\n",
    "    deepfluoro = DeepFluoroDataset(idx)\n",
    "    drr = DRR(\n",
    "        deepfluoro.subject,\n",
    "        deepfluoro.focal_len,\n",
    "        deepfluoro.height // 8,\n",
    "        deepfluoro.delx * 8,\n",
    "    )\n",
    "\n",
    "    if idx == 1:\n",
    "        ct = labelmap_to_mesh(deepfluoro.subject)\n",
    "        ct.clear_cell_data()\n",
    "        plotter.add_mesh(ct)\n",
    "\n",
    "    for img, pose in tqdm(deepfluoro, desc=f\"Subject {idx}\"):\n",
    "        source, target = drr.detector(pose, None)\n",
    "        source = source.squeeze().cpu().detach().numpy()\n",
    "        target = (\n",
    "            target.reshape(drr.detector.height, drr.detector.width, 3)\n",
    "            .cpu()\n",
    "            .detach()\n",
    "            .numpy()\n",
    "        )\n",
    "        principal_ray = pyvista.Line(source, target.mean(axis=0).mean(axis=0))\n",
    "        camera = _make_camera_frustum_mesh(source, target, size=0.05)\n",
    "        plotter.add_mesh(camera, show_edges=False, color=colors[idx - 1])\n",
    "        plotter.add_mesh(principal_ray, line_width=3, color=colors[idx - 1])\n",
    "\n",
    "plotter.export_html(\"renders/deepfluoro.html\")\n",
    "IFrame(\"renders/deepfluoro.html\", height=500, width=749)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
