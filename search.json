[
  {
    "objectID": "ljubljana.html",
    "href": "ljubljana.html",
    "title": "ljubljana",
    "section": "",
    "text": "The Ljubljana dataset is a collection of 2D and 3D digital subtraction angiography (DSA) images from 20 patients undergoing neurovascular procedures at the University of Ljubljana. For each patient there is\n\nOne 3D DSA volume (i.e., rotational angiography)\nTwo 2D DSA acquisitions (one AP and one LAT)\n\nIn total, the dataset comprises 20 3D DSA volumes and 20 2D DSA images.\n\nsource\n\n\n\n LjubljanaDataset (id_number:int, preprocess:bool=True)\n\nA torch.utils.data.Dataset that stores the imaging data for subjects in the Ljubljana dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nid_number\nint\n\nSubject ID in {1, …, 10}\n\n\npreprocess\nbool\nTrue\nConvert X-rays from exponentiated to linear form\n\n\n\n\nsource\n\n\n\n\n Transforms (height:int, width:int, eps:float=1e-06)\n\nStandardize, resize, and normalize X-rays and DRRs before inputting to a deep learning model.\n\n\nCode\nfrom tqdm import tqdm\n\nmean, vars = [], []\nfor idx in tqdm(range(1, 11), ncols=50):\n    specimen = LjubljanaDataset(idx)\n    for img, _, _, _, _, _, _, _, _ in specimen:\n        img = (img - img.min()) / (img.max() - img.min())\n        mean.append(img.mean())\n        vars.append(img.var())\n\nprint(\"Pixel mean :\", sum(mean) / len(mean))\nprint(\"Pixel std dev :\", (sum(vars) / len(vars)).sqrt())\n\n\n100%|█████████████| 10/10 [00:32&lt;00:00,  3.26s/it]\n\n\nPixel mean : tensor(0.0306)\nPixel std dev : tensor(0.0594)",
    "crumbs": [
      "ljubljana"
    ]
  },
  {
    "objectID": "ljubljana.html#dataset",
    "href": "ljubljana.html#dataset",
    "title": "ljubljana",
    "section": "",
    "text": "The Ljubljana dataset is a collection of 2D and 3D digital subtraction angiography (DSA) images from 20 patients undergoing neurovascular procedures at the University of Ljubljana. For each patient there is\n\nOne 3D DSA volume (i.e., rotational angiography)\nTwo 2D DSA acquisitions (one AP and one LAT)\n\nIn total, the dataset comprises 20 3D DSA volumes and 20 2D DSA images.\n\nsource\n\n\n\n LjubljanaDataset (id_number:int, preprocess:bool=True)\n\nA torch.utils.data.Dataset that stores the imaging data for subjects in the Ljubljana dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nid_number\nint\n\nSubject ID in {1, …, 10}\n\n\npreprocess\nbool\nTrue\nConvert X-rays from exponentiated to linear form\n\n\n\n\nsource\n\n\n\n\n Transforms (height:int, width:int, eps:float=1e-06)\n\nStandardize, resize, and normalize X-rays and DRRs before inputting to a deep learning model.\n\n\nCode\nfrom tqdm import tqdm\n\nmean, vars = [], []\nfor idx in tqdm(range(1, 11), ncols=50):\n    specimen = LjubljanaDataset(idx)\n    for img, _, _, _, _, _, _, _, _ in specimen:\n        img = (img - img.min()) / (img.max() - img.min())\n        mean.append(img.mean())\n        vars.append(img.var())\n\nprint(\"Pixel mean :\", sum(mean) / len(mean))\nprint(\"Pixel std dev :\", (sum(vars) / len(vars)).sqrt())\n\n\n100%|█████████████| 10/10 [00:32&lt;00:00,  3.26s/it]\n\n\nPixel mean : tensor(0.0306)\nPixel std dev : tensor(0.0594)",
    "crumbs": [
      "ljubljana"
    ]
  },
  {
    "objectID": "ljubljana.html#basic-functionalities",
    "href": "ljubljana.html#basic-functionalities",
    "title": "ljubljana",
    "section": "Basic functionalities",
    "text": "Basic functionalities\nIn the Ljubljana dataset, 3D fidicial markers were manually annotated along the centerline of many vessels in the neurovasculature. These can be projected into 2D using projective geometry.\n\n\n\n\n\n\nCaution\n\n\n\nCommercially available biplane scanners used in neurosurgery offer users many degrees of freedom, including control over the intrinsic parameters of the system. In particular, the focal length and principal points are different for every image in the Ljubljana dataset. Therefore, we return these intrinsic parameters along with the extrinsic camera pose for every image.\n\n\nFor every DSA acquisition in the Ljubljana dataset, we visualize\n\nThe ground truth image\nA DRR rendered from the ground truth pose\nAn overlay of projected fiducial markers onto the DRR\nThe difference between the ground truth image and the DRR\nThe difference between the DRR and the ground truth image\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom diffdrr.drr import DRR\nfrom diffdrr.pose import RigidTransform\nfrom diffdrr.visualization import plot_drr\nfrom tqdm import tqdm\n\n\nsubsample = 8\n\nfor id_number in range(1, 11):\n    ljubljana = LjubljanaDataset(id_number)\n\n    for gt, pose, focal_len, height, width, delx, dely, x0, y0 in ljubljana:\n        height = height // subsample\n        width = width // subsample\n        delx = delx * subsample\n        dely = dely * subsample\n        transforms = Transforms(height, width)\n\n        drr = DRR(\n            ljubljana.subject,\n            focal_len,\n            height,\n            delx,\n            width,\n            dely,\n            x0,\n            y0,\n            reverse_x_axis=False,\n        )\n\n        img = drr(pose)\n        gt, img = transforms(gt), transforms(img)\n        x = drr.perspective_projection(pose, ljubljana.subject.fiducials)\n\n        fig, axs = plt.subplots(ncols=5, constrained_layout=True)\n        axs = plot_drr(torch.concat([gt, img, img, gt - img, img - gt]), ticks=False, axs=axs)\n        axs[2].scatter(x[0, ..., 0], x[0, ..., 1], s=0.05)\n        plt.show()",
    "crumbs": [
      "ljubljana"
    ]
  },
  {
    "objectID": "ljubljana.html#citations",
    "href": "ljubljana.html#citations",
    "title": "ljubljana",
    "section": "Citations",
    "text": "Citations\nIf you use the Ljubljana dataset in your work, please cite the author’s original publication:\n@article{pernus20133d,\n  title={3D-2D registration of cerebral angiograms: A method and evaluation on clinical images},\n  author={Mitrović, Uros and Spiclin, Ziga and Likar, Bostjan and Pernus, Franjo},\n  journal={IEEE transactions on medical imaging},\n  volume={32},\n  number={8},\n  pages={1550--1563},\n  year={2013},\n  publisher={IEEE}\n}\nIf you find DiffDRR or DiffDRR-Datasets useful for your work, please cite our paper:\n@inproceedings{gopalakrishnan2022fast,\n  title={Fast auto-differentiable digitally reconstructed radiographs for solving inverse problems in intraoperative imaging},\n  author={Gopalakrishnan, Vivek and Golland, Polina},\n  booktitle={Workshop on Clinical Image-Based Procedures},\n  pages={1--11},\n  year={2022},\n  organization={Springer}\n}",
    "crumbs": [
      "ljubljana"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nget_data_home\n\n get_data_home ()\n\nBy default, datasets are saved in ~/diffdrr_data.\n\nsource\n\n\nload_file\n\n load_file (filename:str)\n\nInternal function for loading datasets.\n\nsource\n\n\ndownload_deepfluoro\n\n download_deepfluoro ()\n\n\nsource\n\n\ndownload_ljubljana\n\n download_ljubljana ()",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "deepfluoro.html",
    "href": "deepfluoro.html",
    "title": "deepfluoro",
    "section": "",
    "text": "The DeepFluoro dataset is a collection of pelvic CT and X-ray images from 6 cadaveric subjects from Johns Hopkins University. For each subject, there is\n\nOne 3D CT volume\nOne 3D labelmap for the CT\nBetween 24-111 2D X-ray fluoroscopy images, depending on the subject\n\nIn total, the dataset comprises six CT volumes and 366 X-ray images (with ground truth camera poses).\n\nsource\n\n\n\n DeepFluoroDataset (id_number:int, preprocess:bool=True,\n                    bone_attenuation_multiplier:float=1.0,\n                    labels:int|list=None, batchless:bool=False)\n\nA torch.utils.data.Dataset that stores the imaging data for subjects in the DeepFluoro dataset and provides an iterator over the X-ray fluoroscopy images and associated poses for each subject. Imaging data can be passed to a diffdrr.drr.DRR to renderer DRRs from ground truth camera poses.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nid_number\nint\n\nSubject ID in {1, …, 6}\n\n\npreprocess\nbool\nTrue\nConvert X-rays from exponentiated to linear form\n\n\nbone_attenuation_multiplier\nfloat\n1.0\nScalar multiplier on density of high attenuation voxels (from DiffDRR, see here)\n\n\nlabels\nint | list\nNone\nLabels from the mask of structures to render\n\n\nbatchless\nbool\nFalse\nReturn unbatched images and poses (e.g., to interface with a torch.utils.data.DataLoader)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDeepFluoroDataset.__getitem__ returns a real X-ray image and its associated ground-truth camera pose. This pose is a diffdrr.pose.RigidTransform object that can be passed directly to the renderer.\n\n\n\nsource\n\n\n\n\n preprocess (img, initial_energy=tensor(65487.))\n\nConvert X-ray fluoroscopy from the exponentiated form to the linear form.\nIf the preprocess flag is True, the DeepFluoroDataset will convert raw X-ray fluoroscopy images from their exponentated form to a linear form by recovering the line integral\n\\[L[i,j] = \\log I_0 - \\log I_f[i,j]\\]\nusing the following steps:\n\nRemove edge induced by the collimator\nSmooth the image to make less noisy\nSubtract the log initial energy for each ray\n\nThe ray’s initial energy was estimated by taking the max over all images in the DeepFluoro dataset.\n\nsource\n\n\n\n\n Transforms (height:int, eps:float=1e-06)\n\nStandardize, resize, and normalize X-rays and DRRs before inputting to a deep learning model.\nWe transform X-rays and DRRs before inputting them to a deep learning model by\n\nRescaling pixel values to [0, 1]\nResizing the images to a specified size\nNormalizing pixel values by the dataset mean and standard deviation",
    "crumbs": [
      "deepfluoro"
    ]
  },
  {
    "objectID": "deepfluoro.html#dataset",
    "href": "deepfluoro.html#dataset",
    "title": "deepfluoro",
    "section": "",
    "text": "The DeepFluoro dataset is a collection of pelvic CT and X-ray images from 6 cadaveric subjects from Johns Hopkins University. For each subject, there is\n\nOne 3D CT volume\nOne 3D labelmap for the CT\nBetween 24-111 2D X-ray fluoroscopy images, depending on the subject\n\nIn total, the dataset comprises six CT volumes and 366 X-ray images (with ground truth camera poses).\n\nsource\n\n\n\n DeepFluoroDataset (id_number:int, preprocess:bool=True,\n                    bone_attenuation_multiplier:float=1.0,\n                    labels:int|list=None, batchless:bool=False)\n\nA torch.utils.data.Dataset that stores the imaging data for subjects in the DeepFluoro dataset and provides an iterator over the X-ray fluoroscopy images and associated poses for each subject. Imaging data can be passed to a diffdrr.drr.DRR to renderer DRRs from ground truth camera poses.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nid_number\nint\n\nSubject ID in {1, …, 6}\n\n\npreprocess\nbool\nTrue\nConvert X-rays from exponentiated to linear form\n\n\nbone_attenuation_multiplier\nfloat\n1.0\nScalar multiplier on density of high attenuation voxels (from DiffDRR, see here)\n\n\nlabels\nint | list\nNone\nLabels from the mask of structures to render\n\n\nbatchless\nbool\nFalse\nReturn unbatched images and poses (e.g., to interface with a torch.utils.data.DataLoader)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDeepFluoroDataset.__getitem__ returns a real X-ray image and its associated ground-truth camera pose. This pose is a diffdrr.pose.RigidTransform object that can be passed directly to the renderer.\n\n\n\nsource\n\n\n\n\n preprocess (img, initial_energy=tensor(65487.))\n\nConvert X-ray fluoroscopy from the exponentiated form to the linear form.\nIf the preprocess flag is True, the DeepFluoroDataset will convert raw X-ray fluoroscopy images from their exponentated form to a linear form by recovering the line integral\n\\[L[i,j] = \\log I_0 - \\log I_f[i,j]\\]\nusing the following steps:\n\nRemove edge induced by the collimator\nSmooth the image to make less noisy\nSubtract the log initial energy for each ray\n\nThe ray’s initial energy was estimated by taking the max over all images in the DeepFluoro dataset.\n\nsource\n\n\n\n\n Transforms (height:int, eps:float=1e-06)\n\nStandardize, resize, and normalize X-rays and DRRs before inputting to a deep learning model.\nWe transform X-rays and DRRs before inputting them to a deep learning model by\n\nRescaling pixel values to [0, 1]\nResizing the images to a specified size\nNormalizing pixel values by the dataset mean and standard deviation",
    "crumbs": [
      "deepfluoro"
    ]
  },
  {
    "objectID": "deepfluoro.html#basic-functionalities",
    "href": "deepfluoro.html#basic-functionalities",
    "title": "deepfluoro",
    "section": "Basic functionalities",
    "text": "Basic functionalities\nInitializing a DeepFluoroDataset returns an object containing a torchio.Subject that directly interfaces with the diffdrr.drr.DRR module. Intrinsic parameters (i.e., the imaging system’s focal length, image dimensions, pixel spacings, and principal offsets) are also returned.\n\nimport matplotlib.pyplot as plt\n\nfrom diffdrr.drr import DRR\nfrom diffdrr.visualization import plot_drr\n\n\n\n\n\n\n\nTip\n\n\n\nX-rays in the DeepFluoro dataset are 1536 × 1536 (before preprocessing) with an isotropic pixel spacing of 0.194 mm. To render such large X-rays with a single GPU, we can use the patch_size argument in DiffDRR.\n\n\n\n# Load a subject from the DeepFluoroDataset WITHOUT preprocessing (non-default)\ndeepfluoro = DeepFluoroDataset(id_number=1, preprocess=False)\n\n# Initialize the DRR module\ndrr = DRR(\n    deepfluoro.subject,\n    deepfluoro.focal_len,\n    deepfluoro.height,\n    deepfluoro.delx,\n    x0=deepfluoro.x0,\n    y0=deepfluoro.y0,\n    reverse_x_axis=True,\n    patch_size=deepfluoro.height // 4,\n).cuda()\ntransform = Transforms(deepfluoro.height)\n\n# Render a DRR from the ground truth camera pose\ngt, pose = deepfluoro[0]\nimg = drr(pose.cuda()).cpu()\ngt, img = transform(gt), transform(img)\nplot_drr(torch.concat([gt, img]), title=[\"Raw X-ray\", \"DRR\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nPreprocessing X-rays\nDiffDRR computes the line integral of an X-ray traced over a CT volume. However, X-rays in the DeepFluoro dataset come in the exponentiated form. To convert raw X-rays to a linear form such that they look like our DRRs, we need to\n\nCrop 50 pixels off each edge to remove the effects of the collimator\nInvert the imaging equation to recover the line integral radiograph\n\nFrom the Beer-Lambert Law, the equation governing fluoroscopy images is \\[ I_f[i, j] = I_0 \\exp(-L[i, j]) \\,, \\] where \\(L[i, j]\\) is the line integral of an X-ray through the volume. Inverting this, we recover \\[ L[i, j] = \\log I_0 - \\log I_f[i, j] \\,.\\] where the constant \\(I_0\\) for each image represents the initial energy of each ray. We approximate \\(I_0 = \\max_{i,j} I_f[i,j]\\), assuming that this represents a ray that reached the detector plane without first intersecting the volume.\n\n\nCode\n# Load a subject from the DeepFluoroDataset WITH preprocessing\ndeepfluoro = DeepFluoroDataset(id_number=1)\n\n# Initialize the DRR module\ndrr = DRR(\n    deepfluoro.subject,\n    deepfluoro.focal_len,\n    deepfluoro.height,\n    deepfluoro.delx,\n    x0=deepfluoro.x0,\n    y0=deepfluoro.y0,\n    patch_size=deepfluoro.height // 4,\n).cuda()\ntransform = Transforms(deepfluoro.height)\n\n# Render a DRR from the ground truth camera pose\ngt, pose = deepfluoro[0]\nimg = drr(pose.cuda()).cpu()\ngt, img = transform(gt), transform(img)\nplot_drr(torch.concat([gt, img, gt - img]))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nChanging bone attenuation for DRRs\nWe can preprocess the CT by segmenting air, soft tissue, and bone before generating DRRs.\n\nUsing bone_attenuation_multiplier=1.0 (default) sets the value of air voxels to 0\nIncreasing bone_attenuation_multiplier weights the density of bones higher than that of soft tissue (i.e., increases contrast in the DRR)\n\n\n\n\n\n\n\nTip\n\n\n\nbone_attenuation_multiplier between [1.0, 3.0] seems to work well for most images in this dataset.\n\n\n\n\nCode\n# Load a subject from the DeepFluoroDataset WITH preprocessing\ndeepfluoro = DeepFluoroDataset(id_number=1, bone_attenuation_multiplier=2.5)\n\n# Initialize the DRR module\ndrr = DRR(\n    deepfluoro.subject,\n    deepfluoro.focal_len,\n    deepfluoro.height,\n    deepfluoro.delx,\n    x0=deepfluoro.x0,\n    y0=deepfluoro.y0,\n    patch_size=deepfluoro.height // 4,\n).cuda()\ntransform = Transforms(deepfluoro.height)\n\n# Render a DRR from the ground truth camera pose\ngt, pose = deepfluoro[0]\nimg = drr(pose.cuda()).cpu()\ngt, img = transform(gt), transform(img)\nplot_drr(torch.concat([gt, img]), title=[\"Processed X-ray\", \"DRR\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nNow, DRRs generated from the ground truth C-arm pose looks remarkably similar to the real X-ray!\n\n\nRotated X-ray test\n\n\n\n\n\n\nCaution\n\n\n\nSome X-ray images in the dataset are rotated 180 degrees in-plane. If the X-ray and DRR below are both in the standard orientation, this error in the dataset has been handled properly by the DeepFluoroDataset class.\n\n\n\n\nCode\n# Get an example where the image and pose are rotated\ngt, pose = deepfluoro[34]\nimg = drr(pose.cuda()).cpu()\ngt, img = transform(gt), transform(img)\nplot_drr(torch.concat([gt, img]), title=[\"Processed X-ray\", \"DRR\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nRendering smaller X-rays\nRendering such large X-rays is prohibitively slow for online training and optimization schemes. For these purposes, we can simply downsample the X-rays and associated imaging planes we use to render DRRs.\n\n# Specify a subsampling factor\nsubsample = 4\n\n# Load a subject from the DeepFluoroDataset\ndeepfluoro = DeepFluoroDataset(id_number=1, bone_attenuation_multiplier=2.5)\n\n# Initialize the DRR module WITH subsampling\ndrr = DRR(\n    deepfluoro.subject,\n    deepfluoro.focal_len,\n    deepfluoro.height // subsample,\n    deepfluoro.delx * subsample,\n    x0=deepfluoro.x0,\n    y0=deepfluoro.y0,\n).cuda()\ntransform = Transforms(deepfluoro.height // subsample)\n\n# Render a DRR from the ground truth camera pose\ngt, pose = deepfluoro[0]\nimg = drr(pose.cuda()).cpu()\ngt, img = transform(gt), transform(img)\nplot_drr(torch.concat([gt, img]), title=[\"Downsampled X-ray\", \"DRR\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nProjecting fiducial markers\nIn the DeepFluoro dataset, 3D fiducial markers were digitally placed in the preoperative CT. Projecting these markers into 2D using ground truth and estimated camera matrices can be used to calculate mean Reprojection Error (mRPE), a common evaluation metric for 2D/3D registration.\n\n\n\n\n\n\nNote\n\n\n\nThe output of drr.perspective_projection is in units of pixels and the system’s intrinsic matrix is automatically determined from the intrinsic parameters passed to the diffdrr.drr.DRR constructor.\n\n\n\nx = drr.perspective_projection(pose, deepfluoro.subject.fiducials.cuda()).cpu()\nplot_drr(img)\nplt.scatter(x[0, ..., 0], x[0, ..., 1])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRendering specific structures\nRender a specific subset of anatomical structures using a list of labels. For example, to only render the pelvis and spine, use labels=[1, 2, 3, 4, 7].\n\n# Load a subject from the DeepFluoroDataset\ndeepfluoro = DeepFluoroDataset(id_number=1, bone_attenuation_multiplier=2.5, labels=[1, 2, 3, 4, 7])\nprint(deepfluoro.subject.label_def)\n\n# Initialize the DRR module WITH subsampling\ndrr = DRR(\n    deepfluoro.subject,\n    deepfluoro.focal_len,\n    deepfluoro.height // subsample,\n    deepfluoro.delx * subsample,\n    x0=deepfluoro.x0,\n    y0=deepfluoro.y0,\n).cuda()\ntransform = Transforms(deepfluoro.height // subsample)\n\n# Render a DRR from the ground truth camera pose\ngt, pose = deepfluoro[0]\nimg = drr(pose.cuda()).cpu()\ngt, img = transform(gt), transform(img)\nplot_drr(torch.concat([gt, img]), title=[\"Downsampled X-ray\", \"DRR\"])\nplt.show()\n\n{1: 'left-hemipelvis', 2: 'right-hemipelvis', 3: 'vertebrae', 4: 'upper-sacrum', 5: 'left-femur', 6: 'right-femur', 7: 'lower-sacrum'}",
    "crumbs": [
      "deepfluoro"
    ]
  },
  {
    "objectID": "deepfluoro.html#visualizing-camera-poses-in-3d",
    "href": "deepfluoro.html#visualizing-camera-poses-in-3d",
    "title": "deepfluoro",
    "section": "Visualizing camera poses in 3D",
    "text": "Visualizing camera poses in 3D\nWe can use 3D plotting functions from PyVista available in DiffDRR to visualize the distributions of camera poses for the six subjects and 366 X-rays in the DeepFluoro dataset.\n\nimport pyvista\nfrom IPython.display import IFrame\nfrom tqdm import tqdm\n\nfrom diffdrr.visualization import _make_camera_frustum_mesh, labelmap_to_mesh\n\npyvista.start_xvfb()\n\n\n\nCode\nplotter = pyvista.Plotter()\n\n\ncolors = [\"#66c2a5\", \"#fc8d62\", \"#8da0cb\", \"#e78ac3\", \"#a6d854\", \"#ffd92f\"]\nfor idx in range(1, 7):\n    deepfluoro = DeepFluoroDataset(idx)\n    drr = DRR(\n        deepfluoro.subject,\n        deepfluoro.focal_len,\n        deepfluoro.height // 8,\n        deepfluoro.delx * 8,\n        x0=deepfluoro.x0,\n        y0=deepfluoro.y0,\n    )\n\n    if idx == 1:\n        ct = labelmap_to_mesh(deepfluoro.subject)\n        ct.clear_cell_data()\n        plotter.add_mesh(ct)\n\n    for img, pose in tqdm(deepfluoro, desc=f\"Subject {idx}\"):\n        source, target = drr.detector(pose, None)\n        source = source.squeeze().cpu().detach().numpy()\n        target = (\n            target.reshape(drr.detector.height, drr.detector.width, 3)\n            .cpu()\n            .detach()\n            .numpy()\n        )\n        principal_ray = pyvista.Line(source, target.mean(axis=0).mean(axis=0))\n        camera = _make_camera_frustum_mesh(source, target, size=0.05)\n        plotter.add_mesh(camera, show_edges=True, color=colors[idx - 1])\n        plotter.add_mesh(principal_ray, line_width=3, color=colors[idx - 1])\n\nplotter.export_html(\"deepfluoro_camera_poses.html\")\n\n\nPerforming Labeled Surface Extraction: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████[00:00&lt;00:00]\nSmoothing Mesh using Taubin Smoothing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████[00:01&lt;00:00]\nCleaning: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████[00:00&lt;00:00]\nSubject 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 111/111 [00:11&lt;00:00,  9.52it/s]\nSubject 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:11&lt;00:00,  9.31it/s]\nSubject 3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:02&lt;00:00,  9.47it/s]\nSubject 4: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:05&lt;00:00,  9.35it/s]\nSubject 5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55/55 [00:05&lt;00:00,  9.31it/s]\nSubject 6: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:02&lt;00:00,  9.56it/s]\n\n\nIn this rendering, each rectangular frustum corresponds to the viewing angle for a particular X-ray and each color represents a different subject. For clarity, only the pelvis of the first subject is visualized.\n\nIFrame(\"deepfluoro_camera_poses.html\", height=500, width=749)",
    "crumbs": [
      "deepfluoro"
    ]
  },
  {
    "objectID": "deepfluoro.html#citations",
    "href": "deepfluoro.html#citations",
    "title": "deepfluoro",
    "section": "Citations",
    "text": "Citations\n\n\n\n\n\n\nNote\n\n\n\nThe DeepFluoro dataset and its DiffDRR wrapper contain many other interesting features that are not covered in this tutorial (e.g., 2D and 3D labelmaps for X-ray and CT images). For a complete overview of the data and features available in DeepFluoro, please check out the original repository.\n\n\nIf you use the DeepFluoro dataset in your work, please cite the authors’ original publication:\n@article{grupp2020automatic,\n  title={Automatic annotation of hip anatomy in fluoroscopy for robust and efficient 2D/3D registration},\n  author={Grupp, Robert B and Unberath, Mathias and Gao, Cong and Hegeman, Rachel A and Murphy, Ryan J and Alexander, Clayton P and Otake, Yoshito and McArthur, Benjamin A and Armand, Mehran and Taylor, Russell H},\n  journal={International journal of computer assisted radiology and surgery},\n  volume={15},\n  pages={759--769},\n  year={2020},\n  publisher={Springer}\n}\nIf you find DiffDRR or DiffDRR-Datasets useful for your work, please cite our paper:\n@inproceedings{gopalakrishnan2022fast,\n  title={Fast auto-differentiable digitally reconstructed radiographs for solving inverse problems in intraoperative imaging},\n  author={Gopalakrishnan, Vivek and Golland, Polina},\n  booktitle={Workshop on Clinical Image-Based Procedures},\n  pages={1--11},\n  year={2022},\n  organization={Springer}\n}",
    "crumbs": [
      "deepfluoro"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DiffDRR Datasets",
    "section": "",
    "text": "Open-source 2D/3D registration datasets and dataloaders for DiffDRR",
    "crumbs": [
      "DiffDRR Datasets"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "DiffDRR Datasets",
    "section": "Install",
    "text": "Install\npip install diffdrrdata",
    "crumbs": [
      "DiffDRR Datasets"
    ]
  },
  {
    "objectID": "index.html#diffdrr",
    "href": "index.html#diffdrr",
    "title": "DiffDRR Datasets",
    "section": "DiffDRR",
    "text": "DiffDRR\nDiffDRR is an differentiable X-ray renderer used for solving inverse problems in tomographic imaging. If you find DiffDRR useful in your work, please cite our paper:\n@inproceedings{gopalakrishnan2022fast,\n  title={Fast auto-differentiable digitally reconstructed radiographs for solving inverse problems in intraoperative imaging},\n  author={Gopalakrishnan, Vivek and Golland, Polina},\n  booktitle={Workshop on Clinical Image-Based Procedures},\n  pages={1--11},\n  year={2022},\n  organization={Springer}\n}",
    "crumbs": [
      "DiffDRR Datasets"
    ]
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "DiffDRR Datasets",
    "section": "Datasets",
    "text": "Datasets\nWe provide APIs to load the following open-source datasets into DiffDRR:\n\n\n\nDataset\nAnatomy\n# of Subjects\n# of 2D Images\nCTs\nX-rays\nGT Fiducials\n\n\n\n\nDeepFluoro\npelvis\n6\n366\n✅\n✅\n❌\n\n\nLjubljana\nneurovasculature\n10\n20\n✅\n✅\n✅\n\n\n\nIf you use any of these datasets, please cite the original papers.\n\nDeepFluoro\nDeepFluoro (Grupp et al., 2020) provides paired X-ray fluoroscopy images and CT volume of the pelvis. The data were collected from six cadaveric subjects at John Hopkins University. Ground truth camera poses were estimated with an offline registration process. A visualization of the X-ray / CT pairs in the DeepFluoro dataset is available here.\n@article{grupp2020automatic,\n  title={Automatic annotation of hip anatomy in fluoroscopy for robust and efficient 2D/3D registration},\n  author={Grupp, Robert B and Unberath, Mathias and Gao, Cong and Hegeman, Rachel A and Murphy, Ryan J and Alexander, Clayton P and Otake, Yoshito and McArthur, Benjamin A and Armand, Mehran and Taylor, Russell H},\n  journal={International journal of computer assisted radiology and surgery},\n  volume={15},\n  pages={759--769},\n  year={2020},\n  publisher={Springer}\n}\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom diffdrr.drr import DRR\nfrom diffdrr.visualization import plot_drr\n\nfrom diffdrrdata.deepfluoro import DeepFluoroDataset, Transforms\n\n# Load a subject from the DeepFluoroDataset\ndeepfluoro = DeepFluoroDataset(id_number=1, bone_attenuation_multiplier=2.5)\n\n# Initialize the DRR module\nsubsample = 4\ndrr = DRR(\n    deepfluoro.subject,\n    deepfluoro.focal_len,\n    deepfluoro.height // subsample,\n    deepfluoro.delx * subsample,\n    x0=deepfluoro.x0,\n    y0=deepfluoro.y0,\n)\ntransform = Transforms(deepfluoro.height // subsample)\n\n# Render a DRR from the ground truth camera pose\ngt, pose = deepfluoro[0]\nimg = drr(pose)\ngt, img = transform(gt), transform(img)\nplot_drr(torch.concat([gt, img, gt - img]), title=[\"Downsampled X-ray\", \"DRR\", \"Difference\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLjubljana\nLjubljana (Mitrovic et al., 2013) provides paired 2D/3D digital subtraction angiography (DSA) images. The data were collected from 10 patients undergoing endovascular image-guided interventions at the University of Ljubljana. Ground truth camera poses were estimated by registering surface fiducial markers.\n@article{pernus20133d,\n  title={3D-2D registration of cerebral angiograms: A method and evaluation on clinical images},\n  author={Mitrović, Uros˘ and S˘piclin, Z˘iga and Likar, Bos˘tjan and Pernus˘, Franjo},\n  journal={IEEE transactions on medical imaging},\n  volume={32},\n  number={8},\n  pages={1550--1563},\n  year={2013},\n  publisher={IEEE}\n}\n\nfrom diffdrrdata.ljubljana import LjubljanaDataset, Transforms\n\n# Load a subject from the LjubljanaDataset\nljubljana = LjubljanaDataset(id_number=1)\ngt, pose, focal_len, height, width, delx, dely, x0, y0 = ljubljana[0]\n\n# Initialize the DRR module\nsubsample = 8\ndrr = DRR(\n    ljubljana.subject,\n    focal_len,\n    height // subsample,\n    delx * subsample,\n    width // subsample,\n    dely * subsample,\n    x0=x0,\n    y0=y0,\n    reverse_x_axis=False,\n)\ntransform = Transforms(height // subsample, width // subsample)\n\n# Render a DRR from the ground truth camera pose\nimg = drr(pose)\ngt, img = transform(gt), transform(img)\nplot_drr(torch.concat([gt, img, gt - img]), title=[\"Downsampled X-ray\", \"DRR\", \"Difference\"])\nplt.show()",
    "crumbs": [
      "DiffDRR Datasets"
    ]
  }
]